<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium Web' rel='stylesheet'>
<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
<style type="text/css">
    body {
        font-family: "Computer Modern Sans", "Titillium Web", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-family: "Computer Modern Sans", "Titillium Web", sans-serif;
        font-size:32px;
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
    img {
        max-width: 100%;
        max-height: auto;
    }
    /*img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }*/

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }

    .paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>

<head>
    <title>SynthVSR: Scaling Visual Speech Recognition With Synthetic Supervision</title>
</head>

<body>
<br>
<div style="text-align: center;">
    <span style="font-size:55px">SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision<br></span>
    <br>
    <table align=center width=900px>
        <tr>
            <td align=center>
						  <span style="font-size:30px">
							  <a href="https://liuxubo717.github.io/">Xubo Liu</a><sup>1,2</sup>,
                              <a href="https://www.linkedin.com/in/egor-lakomkin-1032b337/">Egor Lakomkin</a><sup>2</sup>,
                              <a href="https://www.linkedin.com/in/konstantinos-vougioukas-53988548/">Konstantinos Vougioukas</a><sup>2</sup>,
                              <a href="https://mpc001.github.io/">Pingchuan Ma</a><sup>2</sup>,
                              <a href="https://www.robots.ox.ac.uk/~hchen/">Honglie Chen</a><sup>2</sup>,
                              <a href="https://www.linkedin.com/in/ruiming-xie-176aa049/">Ruiming Xie</a><sup>2</sup>,
                              <a href="https://www.linkedin.com/in/mortaza-morrie-doulaty-44824021/">Morrie Doulaty</a><sup>2</sup>,
                              <a href="https://www.linkedin.com/in/niko-moritz-6005a7148/">Niko Moritz</a><sup>2</sup>,
                              <a href="https://www.linkedin.com/in/jachymkolar/">Jáchym Kolář</a><sup>2</sup>,
                              <a href="https://www.linkedin.com/in/stavros-petridis-4609a93/">Stavros Petridis</a><sup>2</sup>,
                              <a href="https://ibug.doc.ic.ac.uk/maja/">Maja Pantic</a><sup>2</sup>, 
                              <a href="https://ai.facebook.com/people/christian-fuegen">Christian Fuegen</a><sup>2</sup>,
						  </span>
            </td>
        </tr>
        <tr>
            <td align="center">
						  <span style="font-size:26px">
							  <sup>1</sup>University of Surrey, <sup>2</sup>Meta AI
						  </span>
                <br>
                <span style="font-size:25px">
                    Accepted at <a href="http://cvpr2023.thecvf.com/">CVPR 2023</a> 
               </span>
                <br>
            </td>
        </tr>
    </table>

    <table align=center width=300px>
        <tr>
<!--            <td align=center width=140px>-->
<!--                <div style="text-align: center;">-->
<!--                    <span style="font-size:22px"><a href='https://github.com/facebookresearch/sound-spaces' target="_blank"> [Code & Data]</a></span>-->
<!--                </div>-->
<!--            </td>-->
            <td align=center width=100px>
                <div style="text-align: center;">
                    <span style="font-size:26px"><a href='https://arxiv.org/abs/2303.17200' target="_blank"> [Paper]</a></span>
                    <span style="font-size:26px"><a href='bibtex.txt' target="_blank"> [Bibtex]</a></span>
<!--                    <span style="font-size:26px"><a href='https://github.com/facebookresearch/sound-spaces/tree/master/ss_baselines/savi' target="_blank"> [Code]</a></span>-->
                </div>
            </td>
        </tr>
        <tr>
    </table>
</div>


<table align=center width=900px>
    <tr>
        <td>
            <br>
            <div style="text-align: center;">
                <img class="round" width="900px" src="synthvsr.png"/>
            </div>
            <br>
        </td>
    </tr>
    <tr>
        <td style="font-size:15pt; text-align:justify">
            Recently reported state-of-the-art results in visual speech recognition (VSR) often rely on increasingly large amounts of video data, 
            while the publicly available transcribed video datasets are limited in size. In this paper, for the first time, we study the potential 
            of leveraging synthetic visual data for VSR. Our method, termed SynthVSR, substantially improves the performance of VSR systems with 
            synthetic lip movements. The key idea behind SynthVSR is to leverage a speech-driven lip animation model that generates lip movements 
            conditioned on the input speech. The speech-driven lip animation model is trained on an unlabeled audio-visual dataset and could be further 
            optimized towards a pre-trained VSR model when labeled videos are available. As plenty of transcribed acoustic data and face images are available, 
            we are able to generate large-scale synthetic data using the proposed lip animation model for semi-supervised VSR training. 
            We evaluate the performance of our approach on the largest public VSR benchmark - Lip Reading Sentences 3 (LRS3). 
            SynthVSR achieves a WER of 43.3% with only 30 hours of real labeled data, outperforming off-the-shelf approaches using thousands of hours of video. 
            The WER is further reduced to 27.9% when using all 438 hours of labeled data from LRS3, which is on par with the state-of-the-art self-supervised AV-HuBERT [1] method. 
            Furthermore, when combined with large-scale pseudo-labeled audio-visual data SynthVSR yields a new state-of-the-art VSR WER of 16.9% using publicly available data only, 
            surpassing the recent state-of-the-art approaches [2, 3] trained with 29 times more non-public machine-transcribed video data (90,000 hours). 
            Finally, we perform extensive ablation studies to understand the effect of each component in our proposed method.
        </td>
    </tr>
</table>

<br>


<section>
    <header style="text-align: center;font-size: 30pt">Presentation at CVPR</header>
    <hr>
    <table align=center width=900px>
        <tr>
            <td style="font-size:15pt; text-align:justify">
                <div style="text-align: center;">
                    8-min presentation at CVPR 2023.
                </div>
            </td>
        </tr>
        <tr>
            <td>
                <div style="text-align: center;">
                    <iframe width="853" height="480" src="https://www.youtube.com/embed/dH9pCHGvh_o" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
            </td>
        </tr>
    </table>
</section>

<br>

<section>
    <header style="text-align: center;font-size: 30pt">Supplementary Video</header>
    <hr>
    <table align=center width=900px>
        <tr>
            <td style="font-size:15pt; text-align:justify">
                <div style="text-align: center;">
                    Synthetic lip movement videos generated from Librispeech and CelebA.
                </div>
            </td>
        </tr>
        <tr>
            <td>
                <div style="text-align: center;">
                    <iframe width="853" height="480" src="https://www.youtube.com/embed/idIYKCzEFMU" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
            </td>
        </tr>
    </table>
</section>

<br>

<section>
    <header style="text-align: center;font-size: 30pt">References</header>
    <hr>
    <table align=center width=900px>
        <tr>
            <td style="font-size:15pt; text-align:justify">
            <span>
                [1] Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrahman Mohamed.
                <b>Learning audio-visual speech representation by masked multimodal cluster prediction</b>.
                In <i>ICLR</i> 2022 
            </span>
            </td>
        </tr>
        <tr>
            <td style="font-size:15pt; text-align:justify">
            <span>
                [2] Dmitriy Serdyuk, Otavio Braga, and Olivier Siohan.
                <b>Transformer-based video front-ends for audio-visual speech
                    recognition</b>.
                In <i>INTERSPEECH</i> 2022 
            </span>
            </td>
        </tr>
        <tr>
            <td style="font-size:15pt; text-align:justify">
            <span>
                [3] Dmitriy Serdyuk, Olivier Siohan, and Otavio de Pinho ForinBraga.
                <b>Audio-visual speech recognition is worth 32x32x8 voxels</b>.
                In <i>ASRU</i> 2021 
            </span>
            </td>
        </tr>
    </table>
</section>
<br>



</body>
</html>